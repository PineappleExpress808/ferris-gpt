{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import os,re\n",
    "import requests\n",
    "import pinecone \n",
    "import pandas as pd\n",
    "import httplib2, urllib\n",
    "import detectron2\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4.element import Comment\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import QAGenerationChain\n",
    "from langchain.document_loaders import OnlinePDFLoader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.document_loaders import UnstructuredPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Scrape -`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links(URL):\n",
    "    http = httplib2.Http()\n",
    "    status, response = http.request(URL)\n",
    "    links = []\n",
    "    for link in BeautifulSoup(response, 'html.parser', parse_only=SoupStrainer('a')):\n",
    "        if link.has_attr('href'):\n",
    "            links.append(link['href'])\n",
    "    links_clean = list(set([l for l in links if \"transcript\" in l]))\n",
    "    filtered_links = [link for link in links_clean if 'https://tim.blog/' in link and any(char.isdigit() for char in link) and 'https://tim.blog/20' in link]\n",
    "    return filtered_links\n",
    "\n",
    "def get_img(URL):\n",
    "    response = requests.get(URL)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    img_tags = soup.find_all('img')\n",
    "    urls = [img['src'] for img in img_tags]\n",
    "    profile_img = [i for i in urls if 'scaled' in i]\n",
    "    return profile_img\n",
    "\n",
    "def get_year_name(img_link):\n",
    "    try:\n",
    "        year = re.findall(r'/(\\d{4})/', img_link)[0]\n",
    "        name = re.search(r'/([^/]+)-Ill', img_link).group(1).lower()\n",
    "        return year, name \n",
    "    except:\n",
    "        print(\"Fail On:\")\n",
    "        print(img_link)\n",
    "        return None, None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Links w/ Imgs (for many) \n",
    "d=pd.DataFrame()\n",
    "for page in range(1,48):\n",
    "    print(\"page %s\"%page)\n",
    "    url = \"https://tim.blog/category/the-tim-ferriss-show-transcripts/page/%s/\"%page\n",
    "    img_links = get_img(url)\n",
    "    tx_links=get_links(url)\n",
    "    for tx_link in tx_links:\n",
    "        d.loc[tx_link,\"transcript\"]=tx_link\n",
    "        d.loc[tx_link,\"page\"]=page\n",
    "    for img_link in img_links:\n",
    "        year, name = get_year_name(img_link)\n",
    "        if year:\n",
    "            for tx_link in tx_links:\n",
    "                if year in tx_link and name in tx_link:\n",
    "                    d.loc[tx_link,\"image\"]=img_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Links \n",
    "def get_links_old(URL):\n",
    "    http = httplib2.Http()\n",
    "    status, response = http.request(URL)\n",
    "    links = []\n",
    "    for link in BeautifulSoup(response, 'html.parser', parse_only=SoupStrainer('a')):\n",
    "        if link.has_attr('href'):\n",
    "            links.append(link['href'])\n",
    "    filtered_links = list(set([link for link in links if 'https://tim.blog/wp-content' in link]))\n",
    "    return filtered_links\n",
    "\n",
    "url = \"https://tim.blog/2018/09/20/all-transcripts-from-the-tim-ferriss-show/\"\n",
    "tx_links_old=get_links_old(url)\n",
    "d_old=pd.DataFrame()\n",
    "d_old['link']=tx_links_old"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Split -`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_img(episode_id,url):\n",
    "    response = requests.get(url)\n",
    "    imgpath=\"public/0%s.jpg\"%episode_id\n",
    "    with open(imgpath, 'wb') as f:\n",
    "        if 'http' not in url:\n",
    "            url = '{}{}'.format(site, url)\n",
    "        response = requests.get(url)\n",
    "        f.write(response.content)\n",
    "\n",
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def text_from_html(body):\n",
    "    soup = BeautifulSoup(body, 'html.parser')\n",
    "    texts = soup.findAll(string=True)\n",
    "    visible_texts = filter(tag_visible, texts)  \n",
    "    return u\" \".join(t.strip() for t in visible_texts)\n",
    "\n",
    "def get_text_and_title(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    # extract the title of the webpage\n",
    "    title = soup.title.text.strip()\n",
    "    # find the <div> element that contains the text under the title\n",
    "    post_content = soup.find('div', class_='entry-content').get_text()\n",
    "    # extract the text content of the <div> element\n",
    "    text = post_content.strip()\n",
    "    return text,title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk size\n",
    "chunks = 1500\n",
    "splits = [ ]\n",
    "metadatas = [ ]\n",
    "for ix in d.index:\n",
    "    try:\n",
    "        save_img(ix,d.loc[ix,'image'])\n",
    "    except:\n",
    "        None\n",
    "    link = d.loc[ix,\"transcript\"]\n",
    "    text,title=get_text_and_title(link)\n",
    "    # Split\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunks, chunk_overlap=50)\n",
    "    texts_recusive = text_splitter.split_text(text)\n",
    "    splits.append(texts_recusive)\n",
    "    print(len(texts_recusive)) \n",
    "    metadata=[{\"source\":str(ix) + \" \" +link,\"id\":str(ix),\"link\":link,\"title\":title} for chunk in texts_recusive]\n",
    "    print(len(metadata)) \n",
    "    metadatas.append(metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_pdf(file):\n",
    "\n",
    "    loader = OnlinePDFLoader(file)\n",
    "    data = loader.load()[0].page_content\n",
    "    title = data.split(\"Show notes and links\")[0]\n",
    "    return data.strip(),title.strip()\n",
    "\n",
    "# Chunk size\n",
    "chunks = 1500\n",
    "splits_old = [ ]\n",
    "metadatas_old = [ ]\n",
    "for ix in d_old.index:\n",
    "    print(ix)\n",
    "    link = d_old.loc[ix,\"link\"]\n",
    "    text,title=parse_pdf(link)\n",
    "    # Split\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunks, chunk_overlap=50)\n",
    "    texts_recusive = text_splitter.split_text(text)\n",
    "    splits_old.append(texts_recusive)\n",
    "    print(len(texts_recusive)) \n",
    "    metadata=[{\"source\":str(ix) + \" \" +link,\"id\":str(ix),\"link\":link,\"title\":title} for chunk in texts_recusive]\n",
    "    print(len(metadata)) \n",
    "    metadatas_old.append(metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the list of lists \n",
    "splits_all = []\n",
    "for sublist in splits:\n",
    "    splits_all.extend(sublist)\n",
    "metadatas_all = []\n",
    "for sublist in metadatas:\n",
    "    metadatas_all.extend(sublist)\n",
    "\n",
    "# Join the list of lists \n",
    "splits_all_old = []\n",
    "for sublist in splits_old:\n",
    "    splits_all_old.extend(sublist)\n",
    "metadatas_all_old = []\n",
    "for sublist in metadatas_old:\n",
    "    metadatas_all_old.extend(sublist)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Build Eval Sets -`\n",
    " \n",
    "* `Note`: I found I needed to reduce the text size for this to work properly (else JSON errors, but have not been able to find offending chars, so it may be a bug or artefact).\n",
    "* `Note`: This takes a while to run (~500 min); we see OAI time-outs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    link = d.loc[\"https://tim.blog/2023/03/17/peter-attia-outlive-transcript/\",\"transcript\"]\n",
    "    text,title=get_text_and_title(link)\n",
    "    t = text[0:int(len(text)/3)]\n",
    "    chain = QAGenerationChain.from_llm(ChatOpenAI(temperature = 0))\n",
    "    qa_attia = chain.run(t)\n",
    "except:\n",
    "    print(\"fail\")\n",
    "\n",
    "try:\n",
    "    link = d.loc[\"https://tim.blog/2023/03/10/dr-andrew-huberman-transcript/\",\"transcript\"]\n",
    "    text,title=get_text_and_title(link)\n",
    "    t = text[0:int(len(text)/3)]\n",
    "    chain = QAGenerationChain.from_llm(ChatOpenAI(temperature = 0))\n",
    "    qa_huberman = chain.run(t)\n",
    "except:\n",
    "    print(\"fail\")\n",
    "\n",
    "try:\n",
    "    link = d.loc[\"https://tim.blog/2023/01/25/bill-gurley-transcript/\",\"transcript\"]\n",
    "    text,title=get_text_and_title(link)\n",
    "    t = text[0:int(len(text)/3)]\n",
    "    chain = QAGenerationChain.from_llm(ChatOpenAI(temperature = 0))\n",
    "    qa_gurley = chain.run(t)\n",
    "except:\n",
    "    print(\"fail\")\n",
    "\n",
    "full_qa = qa_gurley + qa_huberman + qa_attia\n",
    "import json\n",
    "with open('eval_set.json', 'w') as fout:\n",
    "    json.dump(full_qa, fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Embed -`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pinecone\n",
    "os.environ[\"PINECONE_API_KEY\"] = \"66b41af0-4796-4bae-84a0-2409e6babab6\"\n",
    "pinecone.init(\n",
    "    api_key=os.environ.get('PINECONE_API_KEY'),  \n",
    "    environment=\"us-east1-gcp\")\n",
    "index_name = \"ferris-gpt\"\n",
    "embeddings = OpenAIEmbeddings()\n",
    "p = Pinecone.from_existing_index(index_name=index_name,embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add data in chunk to avoid data ingest errors\n",
    "chunk_size = 100\n",
    "last_chunk = 0\n",
    "num_chunks = math.ceil(len(splits_all) / chunk_size)\n",
    "for i in range(last_chunk,num_chunks):\n",
    "    \n",
    "    print(i)\n",
    "    start_time = time.time()\n",
    "    start_idx = i * chunk_size\n",
    "    end_idx = min(start_idx + chunk_size, len(splits_all))\n",
    "    \n",
    "    # Extract the current chunk\n",
    "    current_splits = splits_all[start_idx:end_idx]\n",
    "    current_metadatas = metadatas_all[start_idx:end_idx]\n",
    "    \n",
    "    # Add the current chunk to the vector database\n",
    "    p.add_texts(texts = current_splits, metadatas=current_metadatas)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Elapsed time: {elapsed_time} seconds\")\n",
    "    print(\"--------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add data in chunk to avoid data ingest errors\n",
    "chunk_size = 100\n",
    "last_chunk = 39\n",
    "num_chunks = math.ceil(len(splits_all_old) / chunk_size)\n",
    "for i in range(last_chunk,num_chunks):\n",
    "    \n",
    "    print(i)\n",
    "    start_time = time.time()\n",
    "    start_idx = i * chunk_size\n",
    "    end_idx = min(start_idx + chunk_size, len(splits_all_old))\n",
    "    \n",
    "    # Extract the current chunk\n",
    "    current_splits = splits_all_old[start_idx:end_idx]\n",
    "    current_metadatas = metadatas_all_old[start_idx:end_idx]\n",
    "    \n",
    "    # Add the current chunk to the vector database\n",
    "    p.add_texts(texts = current_splits, metadatas=current_metadatas)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Elapsed time: {elapsed_time} seconds\")\n",
    "    print(\"--------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
