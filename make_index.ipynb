{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os,re\n",
    "import requests\n",
    "import pinecone \n",
    "import pandas as pd\n",
    "import httplib2, urllib\n",
    "from bs4.element import Comment\n",
    "from langchain.vectorstores import Pinecone\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Scrape -`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links(URL):\n",
    "    http = httplib2.Http()\n",
    "    status, response = http.request(URL)\n",
    "    links = []\n",
    "    for link in BeautifulSoup(response, 'html.parser', parse_only=SoupStrainer('a')):\n",
    "        if link.has_attr('href'):\n",
    "            links.append(link['href'])\n",
    "    links_clean = list(set([l for l in links if \"transcript\" in l]))\n",
    "    filtered_links = [link for link in links_clean if 'https://tim.blog/' in link and any(char.isdigit() for char in link) and 'https://tim.blog/20' in link]\n",
    "    return filtered_links\n",
    "\n",
    "def get_img(URL):\n",
    "    response = requests.get(URL)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    img_tags = soup.find_all('img')\n",
    "    urls = [img['src'] for img in img_tags]\n",
    "    profile_img = [i for i in urls if 'scaled' in i]\n",
    "    return profile_img\n",
    "\n",
    "def get_year_name(img_link):\n",
    "    try:\n",
    "        year = re.findall(r'/(\\d{4})/', img_link)[0]\n",
    "        name = re.search(r'/([^/]+)-Ill', img_link).group(1).lower()\n",
    "        return year, name \n",
    "    except:\n",
    "        print(\"Fail On:\")\n",
    "        print(img_link)\n",
    "        return None, None \n",
    "\n",
    "# Store \n",
    "d=pd.DataFrame()\n",
    "for page in range(1,48):\n",
    "    print(\"page %s\"%page)\n",
    "    url = \"https://tim.blog/category/the-tim-ferriss-show-transcripts/page/%s/\"%page\n",
    "    img_links = get_img(url)\n",
    "    tx_links=get_links(url)\n",
    "    for tx_link in tx_links:\n",
    "        d.loc[tx_link,\"transcript\"]=tx_link\n",
    "        d.loc[tx_link,\"page\"]=page\n",
    "    for img_link in img_links:\n",
    "        year, name = get_year_name(img_link)\n",
    "        if year:\n",
    "            for tx_link in tx_links:\n",
    "                if year in tx_link and name in tx_link:\n",
    "                    d.loc[tx_link,\"image\"]=img_link"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Split -`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_img(episode_id,url):\n",
    "    response = requests.get(url)\n",
    "    imgpath=\"public/0%s.jpg\"%episode_id\n",
    "    with open(imgpath, 'wb') as f:\n",
    "        if 'http' not in url:\n",
    "            url = '{}{}'.format(site, url)\n",
    "        response = requests.get(url)\n",
    "        f.write(response.content)\n",
    "\n",
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def text_from_html(body):\n",
    "    soup = BeautifulSoup(body, 'html.parser')\n",
    "    texts = soup.findAll(string=True)\n",
    "    visible_texts = filter(tag_visible, texts)  \n",
    "    return u\" \".join(t.strip() for t in visible_texts)\n",
    "\n",
    "def get_text_and_title(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    # extract the title of the webpage\n",
    "    title = soup.title.text.strip()\n",
    "    # find the <div> element that contains the text under the title\n",
    "    post_content = soup.find('div', class_='entry-content').get_text()\n",
    "    # extract the text content of the <div> element\n",
    "    text = post_content.strip()\n",
    "    return text,title\n",
    "\n",
    "# Chunk size\n",
    "chunks = 1500\n",
    "splits = [ ]\n",
    "metadatas = [ ]\n",
    "for ix in d.index:\n",
    "    try:\n",
    "        save_img(ix,d.loc[ix,'image'])\n",
    "    except:\n",
    "        None\n",
    "    link = d.loc[ix,\"transcript\"]\n",
    "    text,title=get_text_and_title(link)\n",
    "    # Split\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunks, chunk_overlap=50)\n",
    "    texts_recusive = text_splitter.split_text(text)\n",
    "    splits.append(texts_recusive)\n",
    "    print(len(texts_recusive)) \n",
    "    metadata=[{\"source\":str(ix) + \" \" +link,\"id\":str(ix),\"link\":link,\"title\":title} for chunk in texts_recusive]\n",
    "    print(len(metadata)) \n",
    "    metadatas.append(metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the list of lists \n",
    "splits_all = []\n",
    "for sublist in splits:\n",
    "    splits_all.extend(sublist)\n",
    "metadatas_all = []\n",
    "for sublist in metadatas:\n",
    "    metadatas_all.extend(sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(splits_all))\n",
    "print(len(metadatas_all))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Embed -`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pinecone\n",
    "pinecone.init(\n",
    "    api_key=os.environ.get('PINECONE_API_KEY'),  \n",
    "    environment=\"us-east1-gcp\"  \n",
    ")\n",
    "index_name = \"ferris-gpt\"\n",
    "embeddings = OpenAIEmbeddings()\n",
    "p = Pinecone.from_existing_index(index_name=index_name,embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add data in chunk to avoid data ingest errors\n",
    "chunk_size = 100\n",
    "last_chunk = 0\n",
    "num_chunks = math.ceil(len(splits_all) / chunk_size)\n",
    "for i in range(last_chunk,num_chunks):\n",
    "    \n",
    "    print(i)\n",
    "    start_time = time.time()\n",
    "    start_idx = i * chunk_size\n",
    "    end_idx = min(start_idx + chunk_size, len(splits_all))\n",
    "    \n",
    "    # Extract the current chunk\n",
    "    current_splits = splits_all[start_idx:end_idx]\n",
    "    current_metadatas = metadatas_all[start_idx:end_idx]\n",
    "    \n",
    "    # Add the current chunk to the vector database\n",
    "    p.add_texts(texts = current_splits, metadatas=current_metadatas)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Elapsed time: {elapsed_time} seconds\")\n",
    "    print(\"--------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
